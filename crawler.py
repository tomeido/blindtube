from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import re
from fake_useragent import UserAgent
import pandas as pd
import os
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def setup_driver():
    ua = UserAgent()
    options = webdriver.ChromeOptions()

    # ì„±ëŠ¥ ê°œì„  ì„¤ì •
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={ua.random}")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])

    # TLS 1.2+ ê°•ì œ ì‚¬ìš©
    options.add_argument("--ssl-version-min=tls1.2")  
    
    # ì¸ì¦ì„œ ì˜¤ë¥˜ ë¬´ì‹œ (í…ŒìŠ¤íŠ¸ìš©)
    options.add_argument("--ignore-certificate-errors")
    
    # í”„ë¡ì‹œ/ë°©í™”ë²½ íšŒí”¼
    options.add_argument("--disable-features=NetworkService")

    # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê´€ë ¨ ê¸°ëŠ¥ ë¹„í™œì„±í™”
    options.add_argument("--disable-machine-learning-model-downloader")
    options.add_argument("--disable-features=OptimizationHints")
    
    # íƒ€ì„ì•„ì›ƒ ì„¤ì • ê°•í™”
    try:
        driver = webdriver.Chrome(
            service=Service(
                ChromeDriverManager().install(),
                connect_timeout=30,
                keep_alive=True
            ),
            options=options
        )    
    except Exception as e:
        print(f"Error setting up driver: {e}")
        # Fallback to manual driver path
        driver = webdriver.Chrome(service=Service('/path/to/chromedriver'))
    
    driver.set_page_load_timeout(60)  # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ 60ì´ˆ

    return driver

def parse_articles(html):
    soup = BeautifulSoup(html, 'html.parser')
    articles = []
    
    for item in soup.select('div.topic-list.best div.article'):
        topic = item.select_one('span.topic a').text.strip()
        
        title_tag = item.select_one('a.tit')
        title = title_tag.text.strip()
        link = title_tag.get('href')
        if link and not link.startswith('http'):
            link = 'https://www.teamblind.com' + link
        
        has_image = 'ico-img' in title_tag['class'] if title_tag.has_attr('class') else False
        has_poll = 'ico-poll' in title_tag['class'] if title_tag.has_attr('class') else False
        
        like = int(re.search(r'\d+', item.select_one('span.like').text).group())
        comment = int(re.search(r'\d+', item.select_one('a.cmt').text).group())
        
        articles.append({
            'topic': topic,
            'title': title,
            'link' : link,
            'content': "",
            'has_image': has_image,
            'has_poll': has_poll,
            'like': like,
            'comment': comment
        })
    
    return articles

def parse_article_content(driver, url):
    try:
        driver.get(url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        content = soup.select_one('p.contents-txt#contentArea').text.strip()
        
        return {
            'content': content,
        }
    except Exception as e:
        print(f"Error parsing {url}: {str(e)}")
        return None


def crawl_teamblind(url):
    driver = setup_driver()
    retry_count = 0
    max_retries = 3
    
    while retry_count < max_retries:
        try:
            # ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
            session = requests.Session()
            retry = Retry(total=3, backoff_factor=1)
            adapter = HTTPAdapter(max_retries=retry)
            session.mount('http://', adapter)
            session.mount('https://', adapter)
            
            if not session.get(url, timeout=10).ok:
                raise ConnectionError("Failed to establish connection")

            # í˜ì´ì§€ ì ‘ì†
            driver.get(url)
            
            # ëª…ì‹œì  ëŒ€ê¸° ê°•í™”
            WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.topic-list.best")))
            
            # ìŠ¤í¬ë¡¤ ì²˜ë¦¬
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight*0.8);")
            time.sleep(1)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ì¶”ê°€ ë°ì´í„° ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 20).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.article"))
            )
            
            # ê¸°ì¡´ íŒŒì‹± ë¡œì§
            articles = parse_articles(driver.page_source)
            
            results = []
            for article in articles:
                print(f"Processing: {article['link']}")
                article_data = parse_article_content(driver, article['link'])
                if article_data:
                    results.append(article_data)
                    article['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                time.sleep(1)

            for idx, article in enumerate(results):
                articles[idx]['content'] = article['content']
            
            return articles
            
        except Exception as e:
            print(f"Attempt {retry_count+1} failed: {str(e)}")
            retry_count += 1
            time.sleep(5 * retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸°
            
    print(f"Failed after {max_retries} attempts")
    return []


if __name__ == "__main__":
    import time
    from datetime import datetime
    
    CRAWL_INTERVAL = 300  
    
    try:
        while True:
            print(f"\n{'='*50}")
            print(f"í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            articles = crawl_teamblind("https://www.teamblind.com/kr/")
            
            # # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            # for article in articles:
            #     article['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # ì¡°ê±´ í•„í„°ë§: ì¢‹ì•„ìš” 10ê°œ ì´ìƒ, ë‚´ìš© 200ì ì´ìƒ
            filtered_articles = [
                article for article in articles 
                if article['like'] >= 20 and len(article.get('content', '')) >= 200
            ]
            
            script_dir = os.path.dirname(os.path.abspath(__file__))
            output_path = os.path.join(script_dir, "teamblind_articles.xlsx")
            
            if os.path.exists(output_path):
                existing_df = pd.read_excel(output_path, engine='openpyxl')
                new_df = pd.DataFrame(filtered_articles)
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            else:
                combined_df = pd.DataFrame(filtered_articles)
            
            combined_df.drop_duplicates(subset=['link'], keep='first', inplace=True)
            #combined_df.sort_values(by='crawl_time', ascending=False, inplace=True)
            # ìˆ˜ì • ì½”ë“œ (ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸)
            if 'crawl_time' in combined_df.columns:
                combined_df.sort_values(by='crawl_time', ascending=False, inplace=True)
            else:
                print("Warning: crawl_time column not found")
            
            combined_df.to_excel(output_path, index=False, engine='openpyxl')
            
            print(f"\nâœ… ì¡°ê±´ì— ë§ëŠ” ìƒˆë¡œìš´ ê¸€ {len(filtered_articles)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
            print(f"ğŸ“„ ì´ ì €ì¥ ê¸€ ìˆ˜: {len(combined_df)}ê°œ")
            print(f"â° ë‹¤ìŒ í¬ë¡¤ë§ ì˜ˆì •: {datetime.fromtimestamp(time.time() + CRAWL_INTERVAL).strftime('%Y-%m-%d %H:%M:%S')}")
            
            time.sleep(CRAWL_INTERVAL)
            
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")